{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Student Audio_Section2.ipynb","provenance":[{"file_id":"1DdLSv5YglRa6w4n-Kntj6Tb8Rbt1mKbd","timestamp":1624812373718}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c41PaSgeVgk0"},"source":["# Speech to Text Prediction\n","\n","Now that we know the basics of audio signals - graphing them and manipulating them - we are ready to move to the next step in the voice assistant pipeline, which is converting user audio signals into text.\n","\n","<img src=\"https://miro.medium.com/max/1024/1*hvhbk3n9SAcI3yS_D1IuXQ.png\" width=400>\n","\n","This task is difficult because training a model to accurately transcribe an audio signal into textual data faces challenges of background noise, varying pitches and accents, and more which cause variation and inconsistencies in training data."]},{"cell_type":"code","metadata":{"id":"8zffwkKCVeeP","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625414257310,"user_tz":420,"elapsed":11544,"user":{"displayName":"Khioneus Nevula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gio_gol5m8Jc6l4JBZpHylpxGBEpDrRQbHm79BM=s64","userId":"12840458476010149812"}},"outputId":"88e4ac78-0e37-4ac7-c36b-bda1499f01ee"},"source":["#@title Run to download packages\n","import os\n","import librosa\n","import IPython.display as ipd\n","from IPython.display import Audio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.io import wavfile\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import np_utils\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import scipy.signal as signal\n","!pip install torchaudio\n","import torchaudio.functional as funct"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.9.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RW0HYcukx3rl"},"source":["**Speech Recognition**\n","\n","What we are performing in this notebook goes by many names: automatic speech recognition (ASR), computer speech recognition, or speech to text (STT).\n","\n","Some systems are fine-tuned to recognize a specific person's speech, while most are 'speaker independent' systems - which is what we are doing in our model.\n","\n","The field of speech recognition has come a long way - from a single-speaker digit recognition system built in 1952, to sophisticated speaker-independent systems that can fairly accurately recognize any audio signal today.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"waLQMRDbf4dF"},"source":["## Prepare the Data\n","\n","First, we need to mount to our our own personal Google Drive so that we can access the Shared Folder with data.\n","\n","Run the following cell, click the link and copy the long string that appears, and paste it in the box below."]},{"cell_type":"code","metadata":{"id":"mKhu-UZZZl_J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625414281478,"user_tz":420,"elapsed":24170,"user":{"displayName":"Khioneus Nevula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gio_gol5m8Jc6l4JBZpHylpxGBEpDrRQbHm79BM=s64","userId":"12840458476010149812"}},"outputId":"85aca2ef-b39d-45d6-e50e-09402f41481e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y3wX_OPmgU2h"},"source":["We are using [TensorFlow's](https://www.tensorflow.org/datasets/catalog/speech_commands) Speech Commands Dataset. The dataset contains thousands of recordings of roughly 1 second utterances from various people, of twenty distinct words and the digits 0 through 9. We will use sample wav files from all 30 classes, but only a subset of them since it takes time to sample from each audio signal to get the format needed for our models."]},{"cell_type":"code","metadata":{"id":"QJejZ3Vw2iKj","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"ok","timestamp":1625414281686,"user_tz":420,"elapsed":212,"user":{"displayName":"Khioneus Nevula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gio_gol5m8Jc6l4JBZpHylpxGBEpDrRQbHm79BM=s64","userId":"12840458476010149812"}},"outputId":"a4fa28cc-e73f-4ea9-ffd3-e09647a0a41d"},"source":["#@title Run for Video\n","from IPython.display import YouTubeVideo\n","YouTubeVideo('U0XtE4_QLXI')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"400\"\n","            height=\"300\"\n","            src=\"https://www.youtube.com/embed/U0XtE4_QLXI\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.YouTubeVideo at 0x7fea0677ae90>"],"image/jpeg":"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\n"},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"BhCIPBjxTj9u"},"source":["Before running the following code block, follow the link to [this Google Drive folder.](https://drive.google.com/drive/folders/1zWlrNGuoAVRcF0YxGSoVrUhZUOkZGPxZ?usp=sharing) Right-click on the folder name 'Data' and click 'Add shortcut to Drive'. Add the shortcut to 'My Drive'."]},{"cell_type":"code","metadata":{"id":"4oi769ggM62X"},"source":["# Note: this cell can take around 15-20 min to execute\n","# Load in all data as samples\n","local_path = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n","data = np.zeros((2000, 8000)) # all samples for each wav file\n","labels = [] # corresponding ground truth labels\n","\n","row = 0\n","for subfolder in os.listdir(local_path): # loop over the label subdirectories within data folder\n","    for wav_file in os.listdir(local_path + subfolder): # loop over the wav files per subdirectory\n","      filename = local_path + subfolder + \"/\" + wav_file\n","      rate, samples = wavfile.read(filename)\n","      samples = signal.resample(samples, 8000)\n","      data[row, 0:len(samples)] = samples\n","      labels.append(subfolder)\n","      row += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VFKhfBTWxifi"},"source":["### Exercise 1\n","\n","Check the sizes of data and labels. (Hint: what is the type of each?)."]},{"cell_type":"markdown","metadata":{"id":"eBD97gQFZqus"},"source":[""]},{"cell_type":"code","metadata":{"id":"Xc2KZ5OAM8XB"},"source":["### YOUR CODE HERE ###\n","print(data.shape)\n","print(len(labels))\n","print(len(data[0]))\n","\n","print(row)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5ZjrY-6yhuL"},"source":["### Exercise 2\n","\n","Use numpy's [unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function to observe and count the unique labels present in our dataset."]},{"cell_type":"code","metadata":{"id":"GK7sTKo9kW8c"},"source":["# The unique labels will be the words we are predicting from audio.\n","unique_labels = np.unique(np.array(labels.copy()))### YOUR CODE HERE ###        \n","\n","num_unique_labels = len(unique_labels)### YOUR CODE HERE #### \n","print(\"\\n There are {} unique labels.\".format(num_unique_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXgh2jQczNXm"},"source":["### Exercise 3"]},{"cell_type":"code","metadata":{"id":"pDXKpg4oJ9LX"},"source":["# Convert labels into one-hot encoding vectors for multi-class classification.\n","le = LabelEncoder()\n","y = le.fit_transform(labels)\n","y = np_utils.to_categorical(y, num_classes= num_unique_labels)###YOUR CODE HERE ###)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5fTSs2DjC8D"},"source":["**Discuss**: Why do we use one hot encoding vectors as seen above for our labels, as opposed to keeping the labels in text form (i.e. \"bed\", \"bird\", ...) or even as singular values (i.e. 1, 2, ...)?"]},{"cell_type":"markdown","metadata":{"id":"XI0yLq6JhgH6"},"source":["### Example Audio Signal\n","\n","Let's plot the amplitude against time and listen to the corresponding audio, for a single example from our dataset.\n","\n","You can experiment with changing the index (`idx`) in our `data` list of samples from wav files you would like to plot and listen to."]},{"cell_type":"markdown","metadata":{"id":"63jf9IfP1IYx"},"source":["#### Exercise 4"]},{"cell_type":"code","metadata":{"id":"UIqGEdD4Mxmq"},"source":["# Plot amplitude vs time graph.\n","idx = 100 # try changing this\n","# TODO: get the samples and label for the given index audio clip\n","samples = data[idx]### YOUR CODE HERE ###\n","label = labels[idx]### YOUR CODE HERE ###\n","\n","fig = plt.figure(figsize=(14, 8))\n","ax1 = fig.add_subplot(211)\n","ax1.set_title('Raw wave for label: ' + label) # includes groundtruth label\n","ax1.set_xlabel('Time')\n","ax1.set_ylabel('Amplitude')\n","ax1.plot(np.linspace(0, 8000/len(samples), 8000), samples) # sample_rate is 8000 for all\n","wav_filename = os.listdir(local_path + label)[idx]\n","Audio(local_path + label + \"/\" + wav_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--f1OUY-UKnw"},"source":["# Listen to the corresponding audio file.\n","wav_filename = os.listdir(local_path + label)[idx]\n","Audio(local_path + label + \"/\" + wav_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pooWOgEUibER"},"source":["Is there anything interesting about the plot or sound of the audio signal you chose?"]},{"cell_type":"markdown","metadata":{"id":"YR6ym53_qp0n"},"source":["### Separate into train & test\n"]},{"cell_type":"code","metadata":{"id":"seCTgKtFVHKf"},"source":["# Reshape data to have a 2d input (excluding batch size) for model\n","data = np.array(data).reshape(-1,8000,1)\n","print(data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3u4kvAFrfTQ"},"source":["#### Exercise 5\n","\n","We are splitting our 2000 examples into **80% for training** and 20% for testing. \n","\n","\n","Use the `train_test_split` sklearn [function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) below."]},{"cell_type":"code","metadata":{"id":"15kTRvpPUjiq"},"source":["# TODO: use train_test_split with the proper arguments and return values.\n","### YOUR CODE HERE ###\n","x_train, x_test, y_train, y_test = train_test_split(data, y, test_size = 0.33, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qUB4hibjrLNS"},"source":["**Discuss:** What does the `stratify` parameter mean and do for `train_test_split`? (Hint: google around to find out).\n","\n","Why might this be helpful to include?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lGhiwpA_s9jw"},"source":["##Create a Classification Model"]},{"cell_type":"markdown","metadata":{"id":"S2ytNtbgfUb0"},"source":["Let's work our way up to defining our own multi-class classification model from scratch with [Keras](https://keras.io/api/models/model/).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a5LyOT8zuSZD"},"source":["### Convolution Layers\n","\n","\n","You might be familiar with the concept of convolution from computer vision. In that application, images are defined as 2d grids with rows and columns of pixel intensity values. Since they are two-dimensional, we would use a 2d convolution kernel to scan over the image and perform element-wise multiplication and then sum the values together.\n","\n","<img src=\"https://embarc.org/embarc_mli/doc/build/html/_images/image104.jpg\" width=200>\n","\n","In this case, we are using one-dimensional audio signals represented by a list of samples of amplitude values along a timescale. This is why for this application, when we use a convolution neural network we want to use a 1d kernel. In Keras, this is done with a `Conv1d` layer. \n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/4-bit-linear-PCM.svg\" width=200>"]},{"cell_type":"markdown","metadata":{"id":"nPyrIEl2waiR"},"source":["#### Exercise 6\n","\n","Create a sample [Conv1d layer](https://keras.io/api/layers/convolution_layers/convolution1d/) with 4 filters, a kernel size (length of the convolution window) of 12, and ReLU activation function."]},{"cell_type":"code","metadata":{"id":"M7YFC8XXysrI"},"source":["from keras.layers import Conv1D\n","\n","sample_conv_layer = Conv1D(4, 12, activation='relu')### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AUJGjO9lzPo-"},"source":["###MaxPooling Layers\n","\n","MaxPooling is also a downsampling technique used in convolutional neural networks for computer vision, in which the maximum value in a certain image subregion is kept and all other values are discarded.\n","\n","<img src=\"https://qph.fs.quoracdn.net/main-qimg-8afedfb2f82f279781bfefa269bc6a90.webp\" width=300>\n","\n","Why is MaxPooling used?\n","\n","1. Reduce the spatial size of the data --> fewer parameters and faster computation\n","\n","2. Prevent overfitting the training data\n","\n","3. Achieve translational invariance (i.e. recognize same subject even if it's slightly morphed)\n","\n","\n","We can also make use of MaxPooling in CNNs for audio data, and simply use a one-dimensional layer as opposed to two-dimensional.\n","\n","<img src=\"https://peltarion.com/static/1d_global_max_pooling.png\" width=200>"]},{"cell_type":"markdown","metadata":{"id":"F2n9OlID1HON"},"source":["#### Exercise 7\n","\n","Create a sample [MaxPooling1d layer](https://keras.io/api/layers/pooling_layers/max_pooling1d/) with a pooling window of 4 (and a stride equal to the pooling window size)."]},{"cell_type":"code","metadata":{"id":"M2O1st8izY83"},"source":["from keras.layers import MaxPooling1D\n","\n","sample_pool_layer = MaxPooling1D(4,2)### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pcl3zeso3ZJN"},"source":["### Exercise 8\n","\n","What is dropout and why do we use it in AI models?"]},{"cell_type":"markdown","metadata":{"id":"VOcyO2YE3hIB"},"source":["###Our Own Keras Model\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hzb5FCeefXo4"},"source":["#### Exercise 9\n","\n","Finish implementing the following keras model by adding two or more convolution layers (with MaxPool and/or Dropout, as you see fit). \n","\n","Try growing the number of filters & reducing the kernel size for each convolutional layer.\n"]},{"cell_type":"code","metadata":{"id":"DVvGcj5BzeMq"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Input, Conv1D, MaxPooling1D\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","model = Sequential()\n","\n","# First Conv1D layer\n","model.add(Conv1D(8, 12, activation='relu', input_shape=(8000,1)))\n","model.add(MaxPooling1D(3))\n","model.add(Dropout(0.3))\n","\n","# Second Conv1D layer\n","model.add(Conv1D(16, 11, activation='relu'))\n","model.add(MaxPooling1D(3))\n","model.add(Dropout(0.2))\n","\n","### YOUR CODE HERE ###\n","# TO-DO: add 2 more Convolution layers, with MaxPooling & Dropout as needed\n","\n","model.add(Conv1D(32, 10, activation='relu'))\n","model.add(MaxPooling1D(3))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv1D(64, 9, activation='relu'))\n","model.add(MaxPooling1D(3))\n","model.add(Dropout(0.1))\n","#END CODE\n","\n","model.add(Flatten())\n","\n","# Dense Layers\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(len(unique_labels), activation='softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"al_Fs2-_5au3"},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxzouGLa4tSu"},"source":["**Discuss**: Why do we use categorical crossentropy loss?\n","\n","The categorical crossentropy loss function is for when there are two or more label classes (and we have 30). Also, labels are in a one_hot representation. \n","\n","If you have integer labels, you would use the sparse categorical crossentropy loss."]},{"cell_type":"markdown","metadata":{"id":"8L5A6qIC5Mqw"},"source":["#### Early Stopping & Model Checkpoint"]},{"cell_type":"code","metadata":{"id":"wZVcWBrl5B11"},"source":["\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=0.0001) \n","mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V32SBX0i5eH-"},"source":["Determing the number of epochs to train a neural network for is often difficult, and can simply require trial and error. To ease this process, we can make use of a concept called **early stopping**. Early stopping means that you can specify an arbitrary large number of training epochs, but the model will automatically stop training once a performance metric (the `monitor` attribute above) stops improving on the validation (or testing) data. The `mode` attribute specifies if we want to maximize or minimize the metric we pass in for the `monitor` argument.\n","\n","**Discuss**: What do the other arguments do?\n"]},{"cell_type":"markdown","metadata":{"id":"cVsBUvmI7IW9"},"source":["**Model Checkpoint** will save a model or the learned weights at some regular interval, so they can be loaded later to continue the training from the state saved. This is helpful to avoid re-training models from scratch."]},{"cell_type":"markdown","metadata":{"id":"fmOOIuU37_kJ"},"source":["#### Exercise 10\n","\n","Train the model for 100 epochs and a batch size of 32, with the two callback functions defined above (`es` & `mc`).\n","\n","\n","Recall that we train our models with the `.fit()` function, which returns to us a `history` of the training and validation accuracy and loss values."]},{"cell_type":"code","metadata":{"id":"K1X80F6J6WJ3"},"source":["### YOUR CODE HERE ###\n","model.fit(x_train, y_train, batch_size=32,epochs=100,callbacks=[es, mc], validation_data=(x_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kv1g97RWV-hG"},"source":["#### See Predictions"]},{"cell_type":"code","metadata":{"id":"pxkBkQleWIIC"},"source":["# Load the best version of our trained model\n","from keras.models import load_model\n","best_model = load_model('best_model.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwbRdwYTVeAj"},"source":["##### Exercise 11\n","\n","We're defining our own `predict` function which will use our best model to get the predicted probabilities of each of the 5 classes, and output the label corresponding to the highest predicted probability."]},{"cell_type":"code","metadata":{"id":"uS3uRAJhT4AC"},"source":["def predict(audio):\n","  prob = best_model.predict(audio.reshape(1,8000,1))\n","  idx = np.argmax(prob[0])\n","\n","  return unique_labels[idx]### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9ZOag7zVwgk"},"source":["##### Exercise 12\n","Let's see how our model does on testing data!\n","\n","Choose a random audio signal from our testing dataset and output a) the true label, b) the audio clip to listen to, and c) the predicted label."]},{"cell_type":"code","metadata":{"id":"l9bU-3LOUYsO"},"source":["# Listen to random testing datapoint\n","rand_idx = np.random.randint(len(x_test))### YOUR CODE HERE ###\n","samples = x_test[rand_idx].reshape(-1, 8000)\n","print(\"Audio:\", unique_labels[np.argmax(y_test[rand_idx])])\n","\n","# TODO: listen to the audio with the ipd.Audio() function\n","### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DScnRaRCVV3-"},"source":["# Predicted label for the above datapoint\n","print(\"Prediction:\", predict(samples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eSB8oN0kk9PJ"},"source":["**What do you think of our results?** Can you think of any reasons for not-great results, or ways to improve the model?"]},{"cell_type":"markdown","metadata":{"id":"MLDK-zblQ4zK"},"source":["### Why is our model so bad?\n","\n","<img src=\"https://cdn2.hubspot.net/hubfs/4307349/1_Y6VblMTOVg731-8Uygx5eA.jpeg\" width=200>\n","\n","There are a few reasons why the accuracy is so poor (around 20%) for this baseline model.\n","\n","First, we might need a much larger quantity of datapoints, as we only have 2000 training examples. However, uploading and sampling more data would require more time, which would take too long for our experimentation here.\n","\n","Furthermore, we can't really use our 1-D signal of samples directly to train a model.\n","\n","* Speech signals are **quasi-stationary**. This means that they are locally static over a short period of time, but show differences from one local time frame to another.\n","\n","* There is **inter- and intra-speaker variability**, i.e. the same word when spoken by different speakers and the same speaker will give different signals.\n","\n","* There is **speaking rate, pitch, and volume variability** of the signals.\n","\n","Therefore, we typically do **feature extraction** from a 1-D speech signal before feeding it into an AI model. Common features to use are the spectrogram, or MFCCs. (More on MFCCs in the previous notebook).\n"]},{"cell_type":"markdown","metadata":{"id":"MtOGEp46Oo0R"},"source":["## Improve our Model\n","<!-- \n","- bonus: preprocess audio signals like normalize volume, make background quieter, etc -->\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dGRqqeSC_xO6"},"source":["### Mel Spectrograms\n","\n","First, let's create **mel spectrograms** from our data and feed that into our model instead.\n","\n","As a reminder, spectrograms are generated from computing several spectrums by performing Fast Fourier Transform (FFT) on several windowed segments of an audio signal. A spectrogram is a bunch of FFTs stacked on top of each other. It's a way to represent a signal’s amplitude as it varies over time at different frequencies.\n","\n","A mel spectrogram is just a spectrogram converted to be on a mel scale, since humans don't perceive frequencies on a linear scale."]},{"cell_type":"markdown","metadata":{"id":"gBLHse0RI0hr"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"CN07v3qJH8Ky"},"source":["#### Preprocess Data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DvXCV5u7jTKu"},"source":["##### Exercise 13\n","\n","Finish filling in the following to create new training and testing input for our model."]},{"cell_type":"code","metadata":{"id":"wDbLZDCgY32Z"},"source":["# Create new training and testing data, instead of the direct audio samples from before.\n","num_train = x_train.shape[0]\n","num_test = ### YOUR CODE HERE ###\n","\n","x_train_new = np.zeros((num_train, 128, 16))\n","x_test_new = ### YOUR CODE HERE ###\n","\n","# use the melspectrogram from librosa\n","for i in range(num_train):\n","  x_train_new[i,:,:] = librosa.feature.melspectrogram(y=x_train[i].squeeze(), sr=8000)\n","\n","# To-Do: Create x_test_new (hint: look at x_train_new above)\n","### YOUR CODE HERE ###\n","\n","print(x_train_new.shape)\n","print(x_test_new.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muFnnANHZ98x"},"source":["# need to reshape to have a 3d input (ignoring batch size) for model\n","x_train_new = x_train_new.reshape(-1, 128, 16, 1)\n","x_test_new = x_test_new.reshape(-1, 128, 16, 1)\n","\n","print(x_train_new.shape)\n","print(x_test_new.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8W3aagYOeNmQ"},"source":["#### Create Model\n","\n","Now when we create our model, we need to change Conv1D to Conv2D layers, and MaxPoolinng1D to MaxPooling2D layers because the input is 3-d from the spectrograms."]},{"cell_type":"markdown","metadata":{"id":"QidMNCQa8pfA"},"source":["##### Exercise 14\n","\n","Finish filling in the following model."]},{"cell_type":"code","metadata":{"id":"ZFqIHoZuXugD"},"source":["from keras.layers import Conv2D, MaxPooling2D\n","\n","model_spect = ### YOUR CODE HERE ###\n","\n","model_spect.add(Conv2D(4, 8, activation='relu', input_shape=### YOUR CODE HERE ###))\n","model_spect.add(MaxPooling2D(2))\n","\n","model_spect.add(Conv2D(2, 2, activation='relu'))\n","model_spect.add(Dropout(0.1))\n","\n","model_spect.add(Conv2D(2, 2, activation='relu'))\n","\n","model_spect.add(Flatten())\n","\n","model_spect.add(Dense(256, activation='relu'))\n","# TODO: add a second Dense layer with half the units \n","### YOUR CODE HERE ###\n","\n","model_spect.add(Dropout(0.1))\n","# TODO: add the final Dense layer with the correct number of units and activation function\n","### YOUR CODE HERE ###\n","\n","model_spect.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQyVC_zfZn9s"},"source":["model_spect.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cL0OcmIK_S1"},"source":["##### Exercise 15\n","\n","Train the model with `.fit()` (like Exercise 10), using the new training and testing data we just created above. (Hint: `x_train_new` and `x_test_new`)."]},{"cell_type":"code","metadata":{"id":"NCKikpy0WNtW"},"source":["# TODO: ### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enPF5d_1enNa"},"source":["We can see that the model still does quite poorly, but does slightly better than using the direct audio samples."]},{"cell_type":"markdown","metadata":{"id":"3COTq9iU_oVA"},"source":["### MFCCs\n","\n","Next, let's try using **MFCCs** as our features that we feed into the same model architecture as the mel spectrogram version.\n","\n","MFCC stands for Mel Frequency Cepstral Coefficients, and you can learn more about them in the first notebook! Basically, MFCCs features represent phonemes (distinct units of sound)."]},{"cell_type":"markdown","metadata":{"id":"2v9Q1K7AIKJG"},"source":["#### Preprocess Data"]},{"cell_type":"code","metadata":{"id":"eIWxw4R7DOdf"},"source":["# Create new training and testing data, instead of the direct audio samples from before.\n","x_train3 = np.zeros((num_train, 30, 16))\n","x_test3 = np.zeros((num_test, 30, 16))\n","\n","# use the mfcc function from librosa\n","for i in range(num_train):\n","  x_train3[i,:,:] = librosa.feature.mfcc(y=x_train[i].squeeze(), sr=8000, n_mfcc=30)\n","\n","for i in range(num_test):\n","  x_test3[i,:,:] = librosa.feature.mfcc(y=x_test[i].squeeze(), sr=8000, n_mfcc=30)\n","\n","print(x_train3.shape)\n","print(x_test3.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdXuQKxWDtZN"},"source":["# need to reshape to have a 3d input (ignoring batch size) for model\n","x_train3 = x_train3.reshape(-1, 30, 16, 1)\n","x_test3 = x_test3.reshape(-1, 30, 16, 1)\n","\n","print(x_train3.shape)\n","print(x_test3.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_W2wUYA6I6S_"},"source":["#### Create Model"]},{"cell_type":"code","metadata":{"id":"emGWKBPUE47l"},"source":["# Same model as model_spect, with different input_shape.\n","model_mfcc = Sequential()\n","\n","model_mfcc.add(Conv2D(4, 8, activation='relu', input_shape=(30,16,1)))\n","model_mfcc.add(MaxPooling2D(2))\n","\n","model_mfcc.add(Conv2D(2, 2, activation='relu'))\n","model_mfcc.add(Dropout(0.1))\n","\n","model_mfcc.add(Conv2D(2, 2, activation='relu'))\n","\n","model_mfcc.add(Flatten())\n","\n","model_mfcc.add(Dense(256, activation='relu'))\n","model_mfcc.add(Dense(128, activation='relu'))\n","model_mfcc.add(Dropout(0.1))\n","model_mfcc.add(Dense(len(unique_labels), activation='softmax'))\n","\n","model_mfcc.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hblx9iX7FveK"},"source":["model_mfcc.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3rtstzTF1gE"},"source":["model_mfcc.fit(x_train3, y_train, epochs=100, callbacks=[es,mc], \\\n","                    batch_size=32, validation_data=(x_test3, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUF2GkCXGXWe"},"source":["Great! Looks like MFCCs are a great feature option, and raised our model accuracy substantially."]},{"cell_type":"markdown","metadata":{"id":"ose3PRuvfJcV"},"source":["### Exercise 16\n","\n","Try changing the architecture of any of the three models we have to see if you can improve their accuracies.\n","\n","(Hint: experiment with adding more convolution layers, changing convolution argument values, adding or removing dropout layers, changing the optimizer, and so on.)"]},{"cell_type":"markdown","metadata":{"id":"OR4tBzXLfpkn"},"source":["## Test on user input"]},{"cell_type":"code","metadata":{"id":"nPLEujICihSY","cellView":"form"},"source":["#@title Run to enable user input recording\n","\n","!pip install ffmpeg-python\n","\n","# from Noé Tits - Numediart google colab notebook\n","from IPython.display import HTML, Audio\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import numpy as np\n","from scipy.io.wavfile import read as wav_read\n","import io\n","import ffmpeg\n","\n","AUDIO_HTML = \"\"\"\n","<script>\n","var my_div = document.createElement(\"DIV\");\n","var my_p = document.createElement(\"P\");\n","var my_btn = document.createElement(\"BUTTON\");\n","var t = document.createTextNode(\"Press to start recording\");\n","\n","my_btn.appendChild(t);\n","//my_p.appendChild(my_btn);\n","my_div.appendChild(my_btn);\n","document.body.appendChild(my_div);\n","\n","var base64data = 0;\n","var reader;\n","var recorder, gumStream;\n","var recordButton = my_btn;\n","\n","var handleSuccess = function(stream) {\n","  gumStream = stream;\n","  var options = {\n","    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n","    mimeType : 'audio/webm;codecs=opus'\n","    //mimeType : 'audio/webm;codecs=pcm'\n","  };            \n","  //recorder = new MediaRecorder(stream, options);\n","  recorder = new MediaRecorder(stream);\n","  recorder.ondataavailable = function(e) {            \n","    var url = URL.createObjectURL(e.data);\n","    var preview = document.createElement('audio');\n","    preview.controls = true;\n","    preview.src = url;\n","    document.body.appendChild(preview);\n","\n","    reader = new FileReader();\n","    reader.readAsDataURL(e.data); \n","    reader.onloadend = function() {\n","      base64data = reader.result;\n","      //console.log(\"Inside FileReader:\" + base64data);\n","    }\n","  };\n","  recorder.start();\n","  };\n","\n","recordButton.innerText = \"Recording... press to stop\";\n","\n","navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n","\n","\n","function toggleRecording() {\n","  if (recorder && recorder.state == \"recording\") {\n","      recorder.stop();\n","      gumStream.getAudioTracks()[0].stop();\n","      recordButton.innerText = \"Saving the recording... pls wait!\"\n","  }\n","}\n","\n","// https://stackoverflow.com/a/951057\n","function sleep(ms) {\n","  return new Promise(resolve => setTimeout(resolve, ms));\n","}\n","\n","var data = new Promise(resolve=>{\n","//recordButton.addEventListener(\"click\", toggleRecording);\n","recordButton.onclick = ()=>{\n","toggleRecording()\n","\n","sleep(2000).then(() => {\n","  // wait 2000ms for the data to be available...\n","  // ideally this should use something like await...\n","  //console.log(\"Inside data:\" + base64data)\n","  resolve(base64data.toString())\n","\n","});\n","\n","}\n","});\n","      \n","</script>\n","\"\"\"\n","\n","def get_audio():\n","  display(HTML(AUDIO_HTML))\n","  data = eval_js(\"data\")\n","  binary = b64decode(data.split(',')[1])\n","  \n","  process = (ffmpeg\n","    .input('pipe:0')\n","    .output('pipe:1', format='wav')\n","    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n","  )\n","  output, err = process.communicate(input=binary)\n","  \n","  riff_chunk_size = len(output) - 8\n","  # Break up the chunk size into four bytes, held in b.\n","  q = riff_chunk_size\n","  b = []\n","  for i in range(4):\n","      q, r = divmod(q, 256)\n","      b.append(r)\n","\n","  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n","  riff = output[:4] + bytes(b) + output[8:]\n","\n","  sr, audio = wav_read(io.BytesIO(riff))\n","\n","  return audio, sr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUz0K3Oqf2nc"},"source":["# Run and immediately speak a word out loud, then click on button to stop recording.\n","user_audio, sr = get_audio()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2rDZdIljKVO"},"source":["# Save the recording to your files.\n","wavfile.write('user_recording.wav', sr, user_audio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fdydNXUjizK"},"source":["# Read the recording you wrote and resample.\n","rate, user_samples = wavfile.read(\"/content/user_recording.wav\")\n","user_samples = signal.resample(user_samples, 8000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7V4_YLNJbLe"},"source":["### See Predictions"]},{"cell_type":"code","metadata":{"id":"uQ70NCpwkiPE"},"source":["# Predict using first model\n","print(\"First Model Prediction:\", predict(user_samples))\n","\n","print(\"\\n\")\n","\n","# Predict using second model\n","samples_spect = librosa.feature.melspectrogram(y=user_samples, sr=8000)\n","samples_spect = samples_spect.reshape(-1, 128, 16, 1)\n","idx = np.argmax(model_spect.predict(samples_spect)[0])\n","print(\"Second Model Prediction:\", unique_labels[idx])\n","\n","print(\"\\n\")\n","\n","# Predict using third model\n","samples_mfcc = librosa.feature.mfcc(y=user_samples, sr=8000, n_mfcc=30)\n","samples_mfcc = samples_mfcc.reshape(-1, 30, 16, 1)\n","idx = np.argmax(model_mfcc.predict(samples_mfcc)[0])\n","print(\"Third Model Prediction:\", unique_labels[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjCmg1qOJoDs"},"source":["Do you observe anything interesting about the mistakes any of the models make? Do any of the models seem to predict the same words as each other?"]},{"cell_type":"markdown","metadata":{"id":"ghddteHNoGEj"},"source":["## Evaluation methods\n","\n","We've used accuracy in our models to evaluate if they predicted the correct or incorrect word that was spoken. This is straightforward and commonly used. However, there isn't an easy way to tell if our model was *close* to predicting the correct word if it was marked as inaccurate for that prediction (beside inspecting the final softmax probabilities and seeing if there were other contenders with a close high accuracy).\n","\n","It is important to note that the most commonly used metric to evaluate automatic speech recognition (ASR) systems is called **Word Error Rate** (WER). This is for when you are trying to do determine how 'accurate' your prediction is for transcribing a longer audio clip, i.e. more than one word in our model.\n","\n","The Word Error Rate is defined as:\n","\n","`Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken`\n","\n","where \n","- Substitutions are anytime a word gets replaced\n","- Insertions are anytime a word gets added that wasn’t said \n","- Deletions are anytime a word is omitted from the transcript\n","\n","The overall WER for Google is around 18% and Speechmatics is around 21%.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"czkFle52P004"},"source":["## Challenges & Ethics\n","\n","1. **Challenges**\n","- Models require a lot of data to train and perform well, which is an issue we saw with our models in this notebook.\n","\n","- Audio in the real world is very messy, because systems need to accommodate for different background sounds, different volume levels of the target sound the likelihood of echos, and varying accents.\n","\n","- Doing audio processing in real-time also poses challenges since models have to perform well with low-latency, as will the spectrogram or MFCC calculation, and synchronize with the audio buffer thread without any delays.\n","\n","- Creating systems which can transcribe full sentences requires more complex models than ours which just predicted a single word. (Think recurrent neural networks - RNN - and Hidden Markov Models - HMMs - mentioned in the video).\n","\n","2. **Ethics**\n","\n","<img src=\"https://www.pnas.org/content/117/14/7684/F1.large.jpg\" width=400>\n","\n","- There are substantial racial disparities in the average WER of many ASR systems, as seen in the image above. More diverse training datasets are needed to combat this.\n","\n","- For those who have disabilities and rely on assistive technology, issues with ASR systems not understanding them due to their race or accent are epecially problematic.\n","\n","Futher reading on biases: \n","\n","[Scientific American](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/)\n","\n","[Harvard Business Review](https://hbr.org/2019/05/voice-recognition-still-has-significant-race-and-gender-biases)\n","\n","[Built In](https://builtin.com/artificial-intelligence/racial-bias-speech-recognition-systems)\n"]},{"cell_type":"markdown","metadata":{"id":"TE39jaxUl_7S"},"source":["## Advanced Material"]},{"cell_type":"markdown","metadata":{"id":"BKUnp5gzpYs_"},"source":["### RNNs\n","\n","Recurrent Neural Networks (RNNs) are typically used to process *sequential* data like for natural language processing and audio recognition tasks. RNNs contain hidden states which are representations of everything the model has seen so far, since hidden states are continuously updated with new information at each timestep.\n","\n","However, RNNs can face an issue during training known as the **vanishing gradient problem**, which occurs when lots of small numbers are multiplied together (using the chain rule to calculate derivatives) during backpropagation. The model effectively stops learning and updating any weights if its gradient 'vanishes' or goes to 0.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1000/1*lTeIFg5Ecl0hMd3FeNGDYA.png\" width = 350>"]},{"cell_type":"markdown","metadata":{"id":"jlpHOxoKrN-z"},"source":["### LSTMs and GRUs\n","\n","Long short-term memory networks and Gated Recurrent Units are helpful for addressing the vanishing gradient problem associated with RNNs.\n","\n","Both LSTMs and GRUs have internal *gates* which control how information flows through the networks.\n","\n","LSTMs contain 1 - a forget gate, which decides what to keep from previous steps, 2 - an input gate, which decides what relevant information to add from the current step, and 3 - an output gate, which determines what the next hidden state should be.\n","\n","GRUs are similar to LSTMs except they don't have a cell state, and instead use just the hidden states to transfer information. It only contains two gates: 1 - an update gate, and 2 - a reset gate. The update gate decides what information to keep, and what new information to add. The reset gate decides how much past information to forget.\n","\n","GRUs use fewer training parameters and therefore use less memory and execute faster, while LSTMs can be more accurate for longer sequences.\n","\n","<img src=\"https://miro.medium.com/max/3032/1*yBXV9o5q7L_CvY7quJt3WQ.png\" width=400>"]},{"cell_type":"markdown","metadata":{"id":"37NSjAqL66hi"},"source":["#### Exercise 17\n","\n","The update gate in a GRU is similar to what two gates in a LSTM?\n"]},{"cell_type":"markdown","metadata":{"id":"Mjwa8c1lwkb1"},"source":["#### GRU layers in keras\n","\n","The `CuDNNGRU` layer in keras is a fast implementation of a GRU layer, which runs on a GPU. We're using a GRU layer because it's great for sequential data, which is what we're working with.\n","\n","We use the `Bidirectional` wrapper around this layer in the advanced model below, which means that the output layer can get information from past (backwards) and future (forward) states simultaneously.\n"]},{"cell_type":"markdown","metadata":{"id":"46kl2Yuc1Ubk"},"source":["##### Exercise 18\n","\n","Look at the CuDNNGRU layer documentation [here](https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/CuDNNGRU) and create a sample layer with 56 neurons and return the full output sequence (not just the last output)."]},{"cell_type":"code","metadata":{"id":"04Ndi4ATHuc-"},"source":["from tensorflow.compat.v1.keras.layers import CuDNNGRU\n","\n","sample_gru_layer = CuDNNGRU(56, return_sequences=True)### YOUR CODE HERE ###\n","print(sample_gru_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xlxQz3Fwbwh"},"source":["### Batch Normalization\n","\n","Batch normalization applies a transformation that tries to keep the mean output close to 0 and the standard deviation close to 1.\n","\n","This is a technique used to coordinate updates of multiple layers in a model. Standardizing the activations of prior layers means that subsequent layers can make assumptions that the spread and distribution of inputs during the weight update won't change, at least not dramatically. \n","\n","Overall, batch normalization *stabilizes and speeds up* deep neural network training."]},{"cell_type":"markdown","metadata":{"id":"zCDKqkILLNjz"},"source":["### Advanced Model\n","\n","Let's create a more complicated model with a mixture of Convolution layers, GRU layers, and Batch Normalization layers for our speech to text task."]},{"cell_type":"markdown","metadata":{"id":"ZdDtQ2Gm-SsR"},"source":["#### Exercise 19\n","\n","Finish filling in the following model."]},{"cell_type":"code","metadata":{"id":"g1UAYY9HmCZj"},"source":["from keras.layers import BatchNormalization, Bidirectional, MaxPooling1D, Dropout, Flatten, Dense\n","from tensorflow.compat.v1.keras.layers import CuDNNGRU\n","from keras.models import Sequential\n","\n","adv_model = Sequential()\n","\n","adv_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True, input_shape=(8000,1)))\n","\n","# First Conv1D layer\n","adv_model.add(Conv1D(8, 13, padding='valid', activation='relu'))\n","adv_model.add(MaxPooling1D(2))\n","adv_model.add(Dropout(0.3))\n","\n","# Second Conv1D layer\n","adv_model.add(Conv1D(16, 11, padding='valid', activation='relu'))\n","adv_model.add(MaxPooling1D(3))\n","\n","# TODO: Create a third Conv1D layer with 32 filters & kernel_size of 9.\n","# Also, use MaxPooling with a pool_size of 3, & Dropout as you see fit.\n","### YOUR CODE HERE ###\n","\n","adv_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3))\n","\n","# GRU layers\n","adv_model.add(Bidirectional(CuDNNGRU(128, return_sequences=True), merge_mode='sum'))\n","adv_model.add(Bidirectional(CuDNNGRU(128, return_sequences=True), merge_mode='sum'))\n","# TODO: add a final GRU layer with 128 units, but only return the last output.\n","### YOUR CODE HERE ###\n","\n","adv_model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3))\n","\n","# Flatten & Dense layers\n","adv_model.add(Flatten())\n","adv_model.add(Dense(256, activation='relu'))\n","# TODO: add the final Dense layer with correct arguments.\n","### YOUR CODE HERE ###\n","\n","adv_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnCY5YmyMale"},"source":["#### Exercise 20\n","\n","Compile the model with `categorical_crossentropy` loss, `adam` optimizer, and `accuracy` as the metric."]},{"cell_type":"code","metadata":{"id":"UA9YbTZYNQm7"},"source":["### YOUR_CODE_HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkbvPx8P3QDP"},"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint\n","early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=5, min_delta=1e-4) \n","checkpoint = ModelCheckpoint('advanced_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDQmSLKmv1uq"},"source":["hist = adv_model.fit(\n","    x=x_train, \n","    y=y_train,\n","    epochs=60, \n","    callbacks=[early_stop, checkpoint], \n","    batch_size=32, \n","    validation_data=(x_test, y_test)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgfgBw-sNpYV"},"source":["You should observe a validation accuracy of above around 90%! This is a big improvement over our previous models which did not utilize GRU layers, and only used Convolution layers."]},{"cell_type":"markdown","metadata":{"id":"ohADvlcZN7vc"},"source":["#### Plot History\n","\n","We can observe the validation and training loss over the epochs below."]},{"cell_type":"code","metadata":{"id":"JAISkH_F3Bl1"},"source":["plt.plot(hist.history['loss'], label='train')\n","plt.plot(hist.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIjt2n2InufI"},"source":["How do the curves look? Do you see any issues or areas of improvement for the model?"]},{"cell_type":"markdown","metadata":{"id":"6pr48CLJhoqs"},"source":["#### See Predictions\n","\n","Let's use the same method as before to listen to and see the predicted class label for a random testing data audio file."]},{"cell_type":"code","metadata":{"id":"TExUufsFhq22"},"source":["best_model = load_model(\"advanced_model.hdf5\") # redefine best_model\n","\n","rand_idx = np.random.randint(0, len(x_test))\n","samples = x_test[rand_idx].reshape(-1, 8000)\n","\n","print(\"Audio:\", unique_labels[np.argmax(y_test[rand_idx])])\n","ipd.Audio(samples, rate=8000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dDXyl20j2Up"},"source":["print(\"Prediction:\", predict(samples)) # same function we wrote above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yXA6F_a0nOZj"},"source":["#### Exercise 21\n","\n","Change any aspects of our final advanced model, adding or removing layers and altering argument values for layers, to achieve the highest accuracy you can!"]}]}